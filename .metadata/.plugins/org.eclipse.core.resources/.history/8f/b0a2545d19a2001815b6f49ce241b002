package crawler.urlCrawler;

import java.util.ArrayList;

public class MyCrawler {
	private URLQueue readyQ = new URLQueue();
	private URLQueue usedQ = new URLQueue();
	private URLQueue waitQ = new URLQueue();
	private int depth=1;
	String dir = null;

	public MyCrawler(String[] startUrls, int depth,String dir) {
		readyQ.enqueueList(startUrls);
		this.depth = depth;
		this.dir = dir;
	}

	public MyCrawler(String startUrl, int depth, String dir) {
		readyQ.enqueue(startUrl);
		this.depth = depth;
		this.dir = dir;

	}

	public boolean exist(String item) {
		boolean exist = false;
		for(int i=0;i<usedQ.size();i++) {
			String temp = usedQ.get(i);
			if(temp.equals(item)) {exist=true;}
		}
		return exist;
	}

	public void craw() {
		int currentDepth = 1;
		
		//test
		URLQueue t = new URLQueue();
		
		while(readyQ.hasNext() && currentDepth<=depth) {
			while (readyQ.hasNext()) {
				String url = readyQ.dequeue();
				if(!exist(url)) {
					ArrayList<String> urls = Util.getURLs(Util.getSource(url));
					t = readyQ;
					waitQ.enqueueList(urls);
					t = waitQ;
					usedQ.enqueue(url);
					t=usedQ;
				}
			}
			if(!waitQ.hasNext()) {
				System.out.println("No urls from the next depth");
			}
			while(waitQ.hasNext()) {
				String newUrl = waitQ.dequeue();
				t=waitQ;
				readyQ.enqueue(newUrl);
				t=readyQ;
			}
			currentDepth++;
		}
		
		///*
		System.out.println(usedQ.size());
		for(int i=0;i<usedQ.size();i++) {
			System.out.println(usedQ.get(i));
			//Util.saveUrl(usedQ.get(i),dir);
		}
		//*/
		
		System.out.println("Crawling finished");
		
		//save url to txt file
		Util.saveUrls(readyQ.getList(),dir);
		Util.saveUrls(usedQ.getList(),dir);
		System.out.println("Save to "+dir);
	}
}
